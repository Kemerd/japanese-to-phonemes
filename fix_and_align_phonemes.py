#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Fix and align ja_phonemes.json with tokenizer_vocab.json

This script processes the phoneme dictionary to ensure compatibility with
the tokenizer vocabulary and optimal performance:

1. Add missing basic hiragana, katakana (including old forms), and common kanji
2. Convert multi-character IPA sequences to single-character ligatures:
   - d ë ‚Üí  • (voiced alveolo-palatal affricate)
   - t…ï ‚Üí  ® (voiceless alveolo-palatal affricate)
   - ts ‚Üí  ¶ (voiceless alveolar affricate)
   - dz ‚Üí  £ (voiced alveolar affricate)
   - t É ‚Üí  ß (voiceless postalveolar affricate)
   - d í ‚Üí  § (voiced postalveolar affricate)
3. Remove punctuation entries (punctuation passes through unchanged in input)
4. Validate all phoneme outputs use only characters from tokenizer_vocab.json

Usage:
    python fix_and_align_phonemes.py

Input:  original_ja_phonemes.json (backup of original dictionary)
Output: ja_phonemes.json (cleaned and aligned dictionary)
"""

import json
import os
import shutil
import struct
from multiprocessing import Pool, cpu_count
from functools import partial

# Map multi-character IPA to single-character ligatures (from tokenizer vocab)
LIGATURE_MAP = {
    'd ë': ' •',  # U+02A5 - voiced alveolo-palatal affricate
    't…ï': ' ®',  # U+02A8 - voiceless alveolo-palatal affricate  
    'ts': ' ¶',  # U+02A6 - voiceless alveolar affricate
    'dz': ' £',  # U+02A3 - voiced alveolar affricate
    't É': ' ß',  # U+02A7 - voiceless postalveolar affricate
    'd í': ' §',  # U+02A4 - voiced postalveolar affricate
}

# Punctuation that should pass through unchanged (not be in phoneme dict)
PUNCTUATION_TO_REMOVE = {
    '„ÄÇ', '„ÄÅ', 'ÔºÅ', 'Ôºü', 'Ôºö', 'Ôºõ', '„Äå', '„Äç', '„Äé', '„Äè', 
    'Ôºà', 'Ôºâ', '„Éª', '„ÄÄ', '„Äú', '„Çõ', '„Çú',
    '.', ',', '!', '?', ':', ';', '-', '‚Äî', '‚Ä¶',
    '(', ')', '[', ']', '"', "'", ' ', '\n', '\t'
}

# Basic hiragana to add if missing (using IPA ligatures)
BASIC_HIRAGANA = {
    '„ÅÇ': 'a', '„ÅÑ': 'i', '„ÅÜ': '…Ø', '„Åà': 'e', '„Åä': 'o',
    '„Åã': 'ka', '„Åç': 'ki', '„Åè': 'k…Ø', '„Åë': 'ke', '„Åì': 'ko',
    '„Åå': 'ga', '„Åé': 'gi', '„Åê': 'g…Ø', '„Åí': 'ge', '„Åî': 'go',
    '„Åï': 'sa', '„Åó': '…ïi', '„Åô': 's…Ø', '„Åõ': 'se', '„Åù': 'so',
    '„Åñ': 'za', '„Åò': ' •i', '„Åö': 'z…Ø', '„Åú': 'ze', '„Åû': 'zo',
    '„Åü': 'ta', '„Å°': ' ®i', '„Å§': ' ¶…Ø', '„Å¶': 'te', '„Å®': 'to',
    '„Å†': 'da', '„Å¢': ' •i', '„Å•': 'z…Ø', '„Åß': 'de', '„Å©': 'do',
    '„Å™': 'na', '„Å´': 'ni', '„Å¨': 'n…Ø', '„Å≠': 'ne', '„ÅÆ': 'no',
    '„ÅØ': 'ha', '„Å≤': '√ßi', '„Åµ': '…∏…Ø', '„Å∏': 'he', '„Åª': 'ho',
    '„Å∞': 'ba', '„Å≥': 'bi', '„Å∂': 'b…Ø', '„Åπ': 'be', '„Åº': 'bo',
    '„Å±': 'pa', '„Å¥': 'pi', '„Å∑': 'p…Ø', '„Å∫': 'pe', '„ÅΩ': 'po',
    '„Åæ': 'ma', '„Åø': 'mi', '„ÇÄ': 'm…Ø', '„ÇÅ': 'me', '„ÇÇ': 'mo',
    '„ÇÑ': 'ja', '„ÇÜ': 'j…Ø', '„Çà': 'jo',
    '„Çâ': '…æa', '„Çä': '…æi', '„Çã': '…æ…Ø', '„Çå': '…æe', '„Çç': '…æo',
    '„Çè': '…∞a', '„Çê': 'i', '„Çë': 'e', '„Çí': 'o', '„Çì': '…¥',
    '„Çî': 'v…Ø',
    # Small characters
    '„ÅÅ': 'a', '„ÅÉ': 'i', '„ÅÖ': '…Ø', '„Åá': 'e', '„Åâ': 'o',
    '„ÇÉ': 'ja', '„ÇÖ': 'j…Ø', '„Çá': 'jo',
    '„Çé': '…∞a', '„Å£': ' î',
}

# Basic katakana to add if missing (using IPA ligatures)
BASIC_KATAKANA = {
    '„Ç¢': 'a', '„Ç§': 'i', '„Ç¶': '…Ø', '„Ç®': 'e', '„Ç™': 'o',
    '„Ç´': 'ka', '„Ç≠': 'ki', '„ÇØ': 'k…Ø', '„Ç±': 'ke', '„Ç≥': 'ko',
    '„Ç¨': 'ga', '„ÇÆ': 'gi', '„Ç∞': 'g…Ø', '„Ç≤': 'ge', '„Ç¥': 'go',
    '„Çµ': 'sa', '„Ç∑': '…ïi', '„Çπ': 's…Ø', '„Çª': 'se', '„ÇΩ': 'so',
    '„Ç∂': 'za', '„Ç∏': ' •i', '„Ç∫': 'z…Ø', '„Çº': 'ze', '„Çæ': 'zo',
    '„Çø': 'ta', '„ÉÅ': ' ®i', '„ÉÑ': ' ¶…Ø', '„ÉÜ': 'te', '„Éà': 'to',
    '„ÉÄ': 'da', '„ÉÇ': ' •i', '„ÉÖ': 'z…Ø', '„Éá': 'de', '„Éâ': 'do',
    '„Éä': 'na', '„Éã': 'ni', '„Éå': 'n…Ø', '„Éç': 'ne', '„Éé': 'no',
    '„Éè': 'ha', '„Éí': '√ßi', '„Éï': '…∏…Ø', '„Éò': 'he', '„Éõ': 'ho',
    '„Éê': 'ba', '„Éì': 'bi', '„Éñ': 'b…Ø', '„Éô': 'be', '„Éú': 'bo',
    '„Éë': 'pa', '„Éî': 'pi', '„Éó': 'p…Ø', '„Éö': 'pe', '„Éù': 'po',
    '„Éû': 'ma', '„Éü': 'mi', '„É†': 'm…Ø', '„É°': 'me', '„É¢': 'mo',
    '„É§': 'ja', '„É¶': 'j…Ø', '„É®': 'jo',
    '„É©': '…æa', '„É™': '…æi', '„É´': '…æ…Ø', '„É¨': '…æe', '„É≠': '…æo',
    '„ÉØ': '…∞a', '„É∞': 'i', '„É±': 'e', '„É≤': 'o', '„É≥': '…¥',
    '„É¥': 'v…Ø', '„Éµ': 'ka', '„É∂': 'ke',
    # Small characters
    '„Ç°': 'a', '„Ç£': 'i', '„Ç•': '…Ø', '„Çß': 'e', '„Ç©': 'o',
    '„É£': 'ja', '„É•': 'j…Ø', '„Éß': 'jo',
    '„ÉÆ': '…∞a', '„ÉÉ': ' î',
    # Extended katakana
    '„É∑': 'va', '„É∏': 'vi', '„Éπ': 've', '„É∫': 'vo',
}

# Common kanji that should be added
COMMON_KANJI = {
    'Âí≤': 'saki',  # bloom/blossom
}

# ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
# VERB CONJUGATION SYSTEM
# ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

# Godan verb endings and their phoneme representations
GODAN_ENDINGS = {
    '„ÅÜ': '…Ø',      # Ë≤∑„ÅÜ (ka…Ø)
    '„Åè': 'k…Ø',     # Êõ∏„Åè (kak…Ø)
    '„Åê': 'g…Ø',     # Ê≥≥„Åê (ojog…Ø)
    '„Åô': 's…Ø',     # Ë©±„Åô (hanas…Ø)
    '„Å§': ' ¶…Ø',     # ÂæÖ„Å§ (ma ¶…Ø)
    '„Å¨': 'n…Ø',     # Ê≠ª„Å¨ (…ïin…Ø)
    '„Å∂': 'b…Ø',     # È£õ„Å∂ (tob…Ø)
    '„ÇÄ': 'm…Ø',     # Ë™≠„ÇÄ (jom…Ø)
    '„Çã': '…æ…Ø',     # Ëµ∞„Çã (ha…ïi…æ…Ø)
}

# Godan „Å¶-form and „Åü-form phoneme transformations
# Format: ending_sound ‚Üí (te_modification, te_suffix, ta_suffix)
GODAN_TE_TA_MAP = {
    # „Åè ‚Üí „ÅÑ„Å¶/„ÅÑ„Åü
    'k': ('i', 'te', 'ta'),      # Êõ∏„Åè kak…Ø ‚Üí Êõ∏„ÅÑ„Å¶ kaite ‚Üí Êõ∏„ÅÑ„Åü kaita
    # „Åê ‚Üí „ÅÑ„Åß/„ÅÑ„Å†
    'g': ('i', 'de', 'da'),      # Ê≥≥„Åê ojog…Ø ‚Üí Ê≥≥„ÅÑ„Åß ojoide ‚Üí Ê≥≥„ÅÑ„Å† ojoida
    # „Åô ‚Üí „Åó„Å¶/„Åó„Åü
    's': ('…ïi', 'te', 'ta'),     # Ë©±„Åô hanas…Ø ‚Üí Ë©±„Åó„Å¶ hana…ïite ‚Üí Ë©±„Åó„Åü hana…ïita
    # „Å§ ‚Üí „Å£„Å¶/„Å£„Åü
    't': ('tÀê', 'e', 'a'),       # ÂæÖ„Å§ ma ¶…Ø ‚Üí ÂæÖ„Å£„Å¶ matÀêe ‚Üí ÂæÖ„Å£„Åü matÀêa
    # „Å¨ ‚Üí „Çì„Åß/„Çì„Å†
    'n': ('…¥', 'de', 'da'),      # Ê≠ª„Å¨ …ïin…Ø ‚Üí Ê≠ª„Çì„Åß …ïi…¥de ‚Üí Ê≠ª„Çì„Å† …ïi…¥da
    # „Å∂ ‚Üí „Çì„Åß/„Çì„Å†
    'b': ('…¥', 'de', 'da'),      # È£õ„Å∂ tob…Ø ‚Üí È£õ„Çì„Åß to…¥de ‚Üí È£õ„Çì„Å† to…¥da
    # „ÇÄ ‚Üí „Çì„Åß/„Çì„Å†
    'm': ('…¥', 'de', 'da'),      # Ë™≠„ÇÄ jom…Ø ‚Üí Ë™≠„Çì„Åß jo…¥de ‚Üí Ë™≠„Çì„Å† jo…¥da
    # „Çã ‚Üí „Å£„Å¶/„Å£„Åü
    '…æ': ('tÀê', 'e', 'a'),       # Ëµ∞„Çã ha…ïi…æ…Ø ‚Üí Ëµ∞„Å£„Å¶ ha…ïitÀêe ‚Üí Ëµ∞„Å£„Åü ha…ïitÀêa
    # „ÅÜ ‚Üí „Å£„Å¶/„Å£„Åü (…∞ in phonemes)
    '…∞': ('tÀê', 'e', 'a'),       # Ë≤∑„ÅÜ ka…∞ ‚Üí Ë≤∑„Å£„Å¶ katÀêe ‚Üí Ë≤∑„Å£„Åü katÀêa
}

# Text-level „Å¶-form and „Åü-form transformations for godan verbs
GODAN_TE_TA_TEXT = {
    '„Åè': ('„ÅÑ', '„Å¶', '„Åü'),
    '„Åê': ('„ÅÑ', '„Åß', '„Å†'),
    '„Åô': ('„Åó', '„Å¶', '„Åü'),
    '„Å§': ('„Å£', '„Å¶', '„Åü'),
    '„Å¨': ('„Çì', '„Åß', '„Å†'),
    '„Å∂': ('„Çì', '„Åß', '„Å†'),
    '„ÇÄ': ('„Çì', '„Åß', '„Å†'),
    '„Çã': ('„Å£', '„Å¶', '„Åü'),
    '„ÅÜ': ('„Å£', '„Å¶', '„Åü'),
}

# Special irregular verbs with complete conjugation data
IRREGULAR_VERBS = {
    '„Åô„Çã': {
        'type': 'suru',
        'phoneme': 's…Ø…æ…Ø',
        'stems': {
            'text': {'mizen': '„Åó', 'renyou': '„Åó', 'base': '„Åô'},
            'phoneme': {'mizen': '…ïi', 'renyou': '…ïi', 'base': 's…Ø'}
        }
    },
    'Êù•„Çã': {
        'type': 'kuru',
        'phoneme': 'k…Ø…æ…Ø',
        'stems': {
            'text': {'mizen': '„Åì', 'renyou': '„Åç', 'base': '„Åè'},
            'phoneme': {'mizen': 'ko', 'renyou': 'ki', 'base': 'k…Ø'}
        }
    },
    '„Åè„Çã': {
        'type': 'kuru',
        'phoneme': 'k…Ø…æ…Ø',
        'stems': {
            'text': {'mizen': '„Åì', 'renyou': '„Åç', 'base': '„Åè'},
            'phoneme': {'mizen': 'ko', 'renyou': 'ki', 'base': 'k…Ø'}
        }
    },
}

# Verbs that look like ichidan but are actually godan (exceptions)
GODAN_EXCEPTIONS = {
    'Â∏∞„Çã', 'Âàá„Çã', 'Ëµ∞„Çã', 'ÂÖ•„Çã', 'Ë¶Å„Çã', 'Áü•„Çã', 'Ëπ¥„Çã', 'Êªë„Çã',
    'Èôê„Çã', 'Êè°„Çã', 'Á∑¥„Çã', 'Ê∏õ„Çã', 'ÁÑ¶„Çã', 'Ë¶Ü„Çã', 'ÈÅÆ„Çã', 'Êçª„Çã',
}

def detect_verb_type(word, phoneme):
    """
    Detect if word is a verb and classify its type.
    
    Returns:
        str: Verb type ('ichidan', 'godan_X', 'suru', 'suru_compound', 
             'kuru', 'kuru_compound', 'iku', 'aru') or None if not a verb
    """
    # Check for irregular verbs first
    if word in IRREGULAR_VERBS:
        return IRREGULAR_VERBS[word]['type']
    
    # Special verb with exceptional negative form
    if word == '„ÅÇ„Çã':
        return 'aru'
    
    # Special verb with exceptional „Å¶-form
    if word == 'Ë°å„Åè' or word == '„ÅÑ„Åè':
        return 'iku'
    
    # „Åô„Çã-verb compounds (ÂãâÂº∑„Åô„Çã, ÈÅãËª¢„Åô„Çã, etc.)
    if word.endswith('„Åô„Çã') and len(word) > 2:
        return 'suru_compound'
    
    # Êù•„Çã-verb compounds (ÊåÅ„Å£„Å¶Êù•„Çã, etc.)
    if (word.endswith('Êù•„Çã') and len(word) > 2) or (word.endswith('„Åè„Çã') and len(word) > 2):
        return 'kuru_compound'
    
    # Must end in „Çã to be a verb
    if not word.endswith('„Çã'):
        return None
    
    # Check for godan exceptions (verbs that look ichidan but aren't)
    if word in GODAN_EXCEPTIONS:
        return 'godan_ru'
    
    # Ichidan verbs: „Çã preceded by „ÅÑ/„Åà sound in PHONEME
    # This is the key - we check the phoneme, not the text!
    if phoneme.endswith('…æ…Ø'):
        phoneme_stem = phoneme[:-2]  # Remove …æ…Ø
        if len(phoneme_stem) > 0:
            # Check if ends with 'i' or 'e' sound
            if phoneme_stem.endswith('i') or phoneme_stem.endswith('e'):
                return 'ichidan'
    
    # Default to godan „Çã-verb
    return 'godan_ru'


def get_verb_stems(word, phoneme, verb_type):
    """
    Extract verb stems for conjugation from both text and phonemes.
    
    Returns:
        dict: Contains 'text_stem', 'phoneme_stem', 'text_ending', 'phoneme_ending'
    """
    stems = {}
    
    # Handle irregular verbs
    if verb_type in ['suru', 'kuru']:
        irregular_data = IRREGULAR_VERBS.get(word, IRREGULAR_VERBS[word if word in IRREGULAR_VERBS else 'Êù•„Çã'])
        return irregular_data['stems']
    
    # Handle compound verbs
    if verb_type == 'suru_compound':
        # ÂãâÂº∑„Åô„Çã ‚Üí ÂãâÂº∑ + „Åô„Çã stems
        text_prefix = word[:-2]  # Remove „Åô„Çã
        phoneme_prefix = phoneme[:-3]  # Remove s…Ø…æ…Ø
        return {
            'text': {
                'prefix': text_prefix,
                'mizen': text_prefix + '„Åó',
                'renyou': text_prefix + '„Åó',
                'base': text_prefix + '„Åô'
            },
            'phoneme': {
                'prefix': phoneme_prefix,
                'mizen': phoneme_prefix + '…ïi',
                'renyou': phoneme_prefix + '…ïi',
                'base': phoneme_prefix + 's…Ø'
            }
        }
    
    if verb_type == 'kuru_compound':
        # Extract prefix before Êù•„Çã/„Åè„Çã
        if word.endswith('Êù•„Çã'):
            text_prefix = word[:-2]
            phoneme_prefix = phoneme[:-3]
        else:
            text_prefix = word[:-2]
            phoneme_prefix = phoneme[:-3]
        return {
            'text': {
                'prefix': text_prefix,
                'mizen': text_prefix + '„Åì',
                'renyou': text_prefix + '„Åç',
                'base': text_prefix + '„Åè'
            },
            'phoneme': {
                'prefix': phoneme_prefix,
                'mizen': phoneme_prefix + 'ko',
                'renyou': phoneme_prefix + 'ki',
                'base': phoneme_prefix + 'k…Ø'
            }
        }
    
    # Ichidan verbs: remove „Çã (…æ…Ø in phoneme)
    if verb_type == 'ichidan':
        text_stem = word[:-1]  # Remove „Çã
        phoneme_stem = phoneme[:-2]  # Remove …æ…Ø
        return {
            'text': {'stem': text_stem},
            'phoneme': {'stem': phoneme_stem}
        }
    
    # Godan verbs: need to identify the ending consonant
    if verb_type.startswith('godan') or verb_type in ['iku', 'aru']:
        text_ending = word[-1]  # „Åè, „Åê, „Åô, etc.
        
        # Find the consonant in the phoneme
        # For most godan verbs, ending is consonant + …Ø
        if phoneme.endswith('…Ø'):
            if len(phoneme) >= 2:
                phoneme_ending = phoneme[-2]  # The consonant before …Ø
                phoneme_stem = phoneme[:-2]
            else:
                phoneme_ending = '…Ø'
                phoneme_stem = ''
        else:
            # Shouldn't happen for godan, but handle gracefully
            phoneme_ending = phoneme[-1] if phoneme else ''
            phoneme_stem = phoneme[:-1] if len(phoneme) > 1 else ''
        
        text_stem = word[:-1]
        
        return {
            'text': {'stem': text_stem, 'ending': text_ending},
            'phoneme': {'stem': phoneme_stem, 'ending': phoneme_ending}
        }
    
    # Fallback
    return {
        'text': {'stem': word[:-1]},
        'phoneme': {'stem': phoneme[:-2] if phoneme.endswith('…æ…Ø') else phoneme[:-1]}
    }


def generate_conjugations(word, phoneme, verb_type):
    """
    Generate all core conjugation forms for a verb.
    
    Returns:
        dict: Mapping of {conjugated_text: conjugated_phoneme}
    """
    conjugations = {}
    stems = get_verb_stems(word, phoneme, verb_type)
    
    # ============================================================
    # ICHIDAN VERBS (È£ü„Åπ„Çã, Ë¶ã„Çã, etc.)
    # ============================================================
    if verb_type == 'ichidan':
        text_stem = stems['text']['stem']
        phon_stem = stems['phoneme']['stem']
        
        # 1. Past („Åü): È£ü„Åπ„Åü
        conjugations[text_stem + '„Åü'] = phon_stem + 'ta'
        
        # 2. Te-form („Å¶): È£ü„Åπ„Å¶
        conjugations[text_stem + '„Å¶'] = phon_stem + 'te'
        
        # 3. Negative („Å™„ÅÑ): È£ü„Åπ„Å™„ÅÑ
        conjugations[text_stem + '„Å™„ÅÑ'] = phon_stem + 'nai'
        
        # 4. Negative past („Å™„Åã„Å£„Åü): È£ü„Åπ„Å™„Åã„Å£„Åü
        conjugations[text_stem + '„Å™„Åã„Å£„Åü'] = phon_stem + 'nakatta'
        
        # 5. Polite present („Åæ„Åô): È£ü„Åπ„Åæ„Åô
        conjugations[text_stem + '„Åæ„Åô'] = phon_stem + 'mas…Ø'
        
        # 6. Polite past („Åæ„Åó„Åü): È£ü„Åπ„Åæ„Åó„Åü
        conjugations[text_stem + '„Åæ„Åó„Åü'] = phon_stem + 'ma…ïita'
        
        # 7. Polite negative („Åæ„Åõ„Çì): È£ü„Åπ„Åæ„Åõ„Çì
        conjugations[text_stem + '„Åæ„Åõ„Çì'] = phon_stem + 'mase…¥'
        
        # 8. Polite negative past („Åæ„Åõ„Çì„Åß„Åó„Åü): È£ü„Åπ„Åæ„Åõ„Çì„Åß„Åó„Åü
        conjugations[text_stem + '„Åæ„Åõ„Çì„Åß„Åó„Åü'] = phon_stem + 'mase…¥de…ïita'
        
        # 9. Conditional („Å∞): È£ü„Åπ„Çå„Å∞
        conjugations[text_stem + '„Çå„Å∞'] = phon_stem + '…æeba'
        
        # 10. Volitional („Çà„ÅÜ): È£ü„Åπ„Çà„ÅÜ
        conjugations[text_stem + '„Çà„ÅÜ'] = phon_stem + 'jo…Ø'
        
        # 11. Imperative („Çç/„Çà): È£ü„Åπ„Çç
        conjugations[text_stem + '„Çç'] = phon_stem + '…æo'
        conjugations[text_stem + '„Çà'] = phon_stem + 'jo'
        
        # 12. Potential („Çâ„Çå„Çã): È£ü„Åπ„Çâ„Çå„Çã
        conjugations[text_stem + '„Çâ„Çå„Çã'] = phon_stem + '…æa…æe…æ…Ø'
        
        # 13. Passive („Çâ„Çå„Çã): Same as potential for ichidan
        # Already covered above
        
        # 14. Causative („Åï„Åõ„Çã): È£ü„Åπ„Åï„Åõ„Çã
        conjugations[text_stem + '„Åï„Åõ„Çã'] = phon_stem + 'sase…æ…Ø'
        
        # 15. Conditional („Åü„Çâ): È£ü„Åπ„Åü„Çâ
        conjugations[text_stem + '„Åü„Çâ'] = phon_stem + 'ta…æa'
    
    # ============================================================
    # GODAN VERBS (Êõ∏„Åè, Ë©±„Åô, Ë≤∑„ÅÜ, etc.)
    # ============================================================
    elif verb_type.startswith('godan') or verb_type in ['iku', 'aru']:
        text_stem = stems['text']['stem']
        text_ending = stems['text']['ending']
        phon_stem = stems['phoneme']['stem']
        phon_ending = stems['phoneme']['ending']
        
        # Special case: Ë°å„Åè has irregular „Å¶-form
        if verb_type == 'iku':
            # Ë°å„ÅÑ„Å¶ ‚Üí Ë°å„Å£„Å¶ (itte not iite)
            conjugations[text_stem + '„Å£„Åü'] = phon_stem + 'itÀêa'
            conjugations[text_stem + '„Å£„Å¶'] = phon_stem + 'itÀêe'
            conjugations[text_stem + '„Å£„Åü„Çâ'] = phon_stem + 'itÀêa…æa'
        else:
            # Normal godan „Å¶/„Åü forms
            if text_ending in GODAN_TE_TA_TEXT and phon_ending in GODAN_TE_TA_MAP:
                te_mod_text, te_suff_text, ta_suff_text = GODAN_TE_TA_TEXT[text_ending]
                te_mod_phon, te_suff_phon, ta_suff_phon = GODAN_TE_TA_MAP[phon_ending]
                
                # 1. Past („Åü): Êõ∏„ÅÑ„Åü
                conjugations[text_stem + te_mod_text + ta_suff_text] = phon_stem + te_mod_phon + ta_suff_phon
                
                # 2. Te-form („Å¶): Êõ∏„ÅÑ„Å¶
                conjugations[text_stem + te_mod_text + te_suff_text] = phon_stem + te_mod_phon + te_suff_phon
                
                # 15. Conditional („Åü„Çâ): Êõ∏„ÅÑ„Åü„Çâ
                conjugations[text_stem + te_mod_text + ta_suff_text + '„Çâ'] = phon_stem + te_mod_phon + ta_suff_phon + '…æa'
        
        # Get „ÅÇ-row character for negative stem (Êú™ÁÑ∂ÂΩ¢)
        # „Åè‚Üí„Åã, „Åê‚Üí„Åå, „Åô‚Üí„Åï, etc.
        a_row_map = {
            '„Åè': ('„Åã', 'ka'), '„Åê': ('„Åå', 'ga'), '„Åô': ('„Åï', 'sa'),
            '„Å§': ('„Åü', 'ta'), '„Å¨': ('„Å™', 'na'), '„Å∂': ('„Å∞', 'ba'),
            '„ÇÄ': ('„Åæ', 'ma'), '„Çã': ('„Çâ', '…æa'), '„ÅÜ': ('„Çè', '…∞a')
        }
        
        # Special case: „ÅÇ„Çã ‚Üí „Å™„ÅÑ (not „ÅÇ„Çâ„Å™„ÅÑ)
        if verb_type == 'aru':
            conjugations['„Å™„ÅÑ'] = 'nai'
            conjugations['„Å™„Åã„Å£„Åü'] = 'nakatta'
        else:
            if text_ending in a_row_map:
                a_text, a_phon = a_row_map[text_ending]
                
                # 3. Negative („Å™„ÅÑ): Êõ∏„Åã„Å™„ÅÑ
                conjugations[text_stem + a_text + '„Å™„ÅÑ'] = phon_stem + a_phon + 'nai'
                
                # 4. Negative past („Å™„Åã„Å£„Åü): Êõ∏„Åã„Å™„Åã„Å£„Åü
                conjugations[text_stem + a_text + '„Å™„Åã„Å£„Åü'] = phon_stem + a_phon + 'nakatta'
                
                # 14. Causative („Åõ„Çã): Êõ∏„Åã„Åõ„Çã
                conjugations[text_stem + a_text + '„Åõ„Çã'] = phon_stem + a_phon + 'se…æ…Ø'
        
        # Get „ÅÑ-row character for polite stem (ÈÄ£Áî®ÂΩ¢)
        i_row_map = {
            '„Åè': ('„Åç', 'ki'), '„Åê': ('„Åé', 'gi'), '„Åô': ('„Åó', '…ïi'),
            '„Å§': ('„Å°', ' ®i'), '„Å¨': ('„Å´', 'ni'), '„Å∂': ('„Å≥', 'bi'),
            '„ÇÄ': ('„Åø', 'mi'), '„Çã': ('„Çä', '…æi'), '„ÅÜ': ('„ÅÑ', 'i')
        }
        
        if text_ending in i_row_map:
            i_text, i_phon = i_row_map[text_ending]
            
            # 5. Polite present („Åæ„Åô): Êõ∏„Åç„Åæ„Åô
            conjugations[text_stem + i_text + '„Åæ„Åô'] = phon_stem + i_phon + 'mas…Ø'
            
            # 6. Polite past („Åæ„Åó„Åü): Êõ∏„Åç„Åæ„Åó„Åü
            conjugations[text_stem + i_text + '„Åæ„Åó„Åü'] = phon_stem + i_phon + 'ma…ïita'
            
            # 7. Polite negative („Åæ„Åõ„Çì): Êõ∏„Åç„Åæ„Åõ„Çì
            conjugations[text_stem + i_text + '„Åæ„Åõ„Çì'] = phon_stem + i_phon + 'mase…¥'
            
            # 8. Polite negative past („Åæ„Åõ„Çì„Åß„Åó„Åü): Êõ∏„Åç„Åæ„Åõ„Çì„Åß„Åó„Åü
            conjugations[text_stem + i_text + '„Åæ„Åõ„Çì„Åß„Åó„Åü'] = phon_stem + i_phon + 'mase…¥de…ïita'
        
        # Get „Åà-row character for conditional/potential (‰ªÆÂÆöÂΩ¢/ÂèØËÉΩÂΩ¢)
        e_row_map = {
            '„Åè': ('„Åë', 'ke'), '„Åê': ('„Åí', 'ge'), '„Åô': ('„Åõ', 'se'),
            '„Å§': ('„Å¶', 'te'), '„Å¨': ('„Å≠', 'ne'), '„Å∂': ('„Åπ', 'be'),
            '„ÇÄ': ('„ÇÅ', 'me'), '„Çã': ('„Çå', '…æe'), '„ÅÜ': ('„Åà', 'e')
        }
        
        if text_ending in e_row_map:
            e_text, e_phon = e_row_map[text_ending]
            
            # 9. Conditional („Å∞): Êõ∏„Åë„Å∞
            conjugations[text_stem + e_text + '„Å∞'] = phon_stem + e_phon + 'ba'
            
            # 11. Imperative (ÂëΩ‰ª§ÂΩ¢): Êõ∏„Åë
            conjugations[text_stem + e_text] = phon_stem + e_phon
            
            # 12. Potential („Çâ„Çå„Çã): Êõ∏„Åë„Çã
            conjugations[text_stem + e_text + '„Çã'] = phon_stem + e_phon + '…æ…Ø'
        
        # Get „ÅÇ-row for passive (ÂèóË∫´ÂΩ¢)
        if text_ending in a_row_map:
            a_text, a_phon = a_row_map[text_ending]
            
            # 13. Passive („Çâ„Çå„Çã): Êõ∏„Åã„Çå„Çã
            conjugations[text_stem + a_text + '„Çå„Çã'] = phon_stem + a_phon + '…æe…æ…Ø'
        
        # Get „Åä-row for volitional (ÊÑèÂêëÂΩ¢)
        o_row_map = {
            '„Åè': ('„Åì', 'ko'), '„Åê': ('„Åî', 'go'), '„Åô': ('„Åù', 'so'),
            '„Å§': ('„Å®', 'to'), '„Å¨': ('„ÅÆ', 'no'), '„Å∂': ('„Åº', 'bo'),
            '„ÇÄ': ('„ÇÇ', 'mo'), '„Çã': ('„Çç', '…æo'), '„ÅÜ': ('„Åä', 'o')
        }
        
        if text_ending in o_row_map:
            o_text, o_phon = o_row_map[text_ending]
            
            # 10. Volitional („Çà„ÅÜ): Êõ∏„Åì„ÅÜ
            conjugations[text_stem + o_text + '„ÅÜ'] = phon_stem + o_phon + '…Ø'
    
    # ============================================================
    # IRREGULAR VERBS („Åô„Çã, Êù•„Çã)
    # ============================================================
    elif verb_type == 'suru':
        # „Åô„Çã conjugations
        conjugations['„Åó„Åü'] = '…ïita'
        conjugations['„Åó„Å¶'] = '…ïite'
        conjugations['„Åó„Å™„ÅÑ'] = '…ïinai'
        conjugations['„Åó„Å™„Åã„Å£„Åü'] = '…ïinakatta'
        conjugations['„Åó„Åæ„Åô'] = '…ïimas…Ø'
        conjugations['„Åó„Åæ„Åó„Åü'] = '…ïima…ïita'
        conjugations['„Åó„Åæ„Åõ„Çì'] = '…ïimase…¥'
        conjugations['„Åó„Åæ„Åõ„Çì„Åß„Åó„Åü'] = '…ïimase…¥de…ïita'
        conjugations['„Åô„Çå„Å∞'] = 's…Ø…æeba'
        conjugations['„Åó„Çà„ÅÜ'] = '…ïijo…Ø'
        conjugations['„Åó„Çç'] = '…ïi…æo'
        conjugations['„Åõ„Çà'] = 'sejo'
        conjugations['„Åß„Åç„Çã'] = 'deki…æ…Ø'  # Potential form
        conjugations['„Åï„Çå„Çã'] = 'sa…æe…æ…Ø'  # Passive
        conjugations['„Åï„Åõ„Çã'] = 'sase…æ…Ø'  # Causative
        conjugations['„Åó„Åü„Çâ'] = '…ïita…æa'
    
    elif verb_type == 'kuru':
        # Êù•„Çã conjugations
        conjugations['Êù•„Åü'] = 'kita'
        conjugations['„Åç„Åü'] = 'kita'
        conjugations['Êù•„Å¶'] = 'kite'
        conjugations['„Åç„Å¶'] = 'kite'
        conjugations['Êù•„Å™„ÅÑ'] = 'konai'
        conjugations['„Åì„Å™„ÅÑ'] = 'konai'
        conjugations['Êù•„Å™„Åã„Å£„Åü'] = 'konakatta'
        conjugations['„Åì„Å™„Åã„Å£„Åü'] = 'konakatta'
        conjugations['Êù•„Åæ„Åô'] = 'kimas…Ø'
        conjugations['„Åç„Åæ„Åô'] = 'kimas…Ø'
        conjugations['Êù•„Åæ„Åó„Åü'] = 'kima…ïita'
        conjugations['„Åç„Åæ„Åó„Åü'] = 'kima…ïita'
        conjugations['Êù•„Åæ„Åõ„Çì'] = 'kimase…¥'
        conjugations['„Åç„Åæ„Åõ„Çì'] = 'kimase…¥'
        conjugations['Êù•„Åæ„Åõ„Çì„Åß„Åó„Åü'] = 'kimase…¥de…ïita'
        conjugations['„Åç„Åæ„Åõ„Çì„Åß„Åó„Åü'] = 'kimase…¥de…ïita'
        conjugations['Êù•„Çå„Å∞'] = 'k…Ø…æeba'
        conjugations['„Åè„Çå„Å∞'] = 'k…Ø…æeba'
        conjugations['Êù•„Çà„ÅÜ'] = 'kojo…Ø'
        conjugations['„Åì„Çà„ÅÜ'] = 'kojo…Ø'
        conjugations['Êù•„ÅÑ'] = 'koi'
        conjugations['„Åì„ÅÑ'] = 'koi'
        conjugations['Êù•„Çâ„Çå„Çã'] = 'ko…æa…æe…æ…Ø'
        conjugations['„Åì„Çâ„Çå„Çã'] = 'ko…æa…æe…æ…Ø'
        conjugations['Êù•„Åï„Åõ„Çã'] = 'kisase…æ…Ø'
        conjugations['„Åì„Åï„Åõ„Çã'] = 'kosase…æ…Ø'
        conjugations['Êù•„Åü„Çâ'] = 'kita…æa'
        conjugations['„Åç„Åü„Çâ'] = 'kita…æa'
    
    # ============================================================
    # COMPOUND VERBS (ÂãâÂº∑„Åô„Çã, ÊåÅ„Å£„Å¶Êù•„Çã)
    # ============================================================
    elif verb_type == 'suru_compound':
        text_prefix = stems['text']['prefix']
        phon_prefix = stems['phoneme']['prefix']
        
        # Generate all „Åô„Çã forms with prefix
        conjugations[text_prefix + '„Åó„Åü'] = phon_prefix + '…ïita'
        conjugations[text_prefix + '„Åó„Å¶'] = phon_prefix + '…ïite'
        conjugations[text_prefix + '„Åó„Å™„ÅÑ'] = phon_prefix + '…ïinai'
        conjugations[text_prefix + '„Åó„Å™„Åã„Å£„Åü'] = phon_prefix + '…ïinakatta'
        conjugations[text_prefix + '„Åó„Åæ„Åô'] = phon_prefix + '…ïimas…Ø'
        conjugations[text_prefix + '„Åó„Åæ„Åó„Åü'] = phon_prefix + '…ïima…ïita'
        conjugations[text_prefix + '„Åó„Åæ„Åõ„Çì'] = phon_prefix + '…ïimase…¥'
        conjugations[text_prefix + '„Åó„Åæ„Åõ„Çì„Åß„Åó„Åü'] = phon_prefix + '…ïimase…¥de…ïita'
        conjugations[text_prefix + '„Åô„Çå„Å∞'] = phon_prefix + 's…Ø…æeba'
        conjugations[text_prefix + '„Åó„Çà„ÅÜ'] = phon_prefix + '…ïijo…Ø'
        conjugations[text_prefix + '„Åó„Çç'] = phon_prefix + '…ïi…æo'
        conjugations[text_prefix + '„Åõ„Çà'] = phon_prefix + 'sejo'
        conjugations[text_prefix + '„Åß„Åç„Çã'] = phon_prefix + 'deki…æ…Ø'
        conjugations[text_prefix + '„Åï„Çå„Çã'] = phon_prefix + 'sa…æe…æ…Ø'
        conjugations[text_prefix + '„Åï„Åõ„Çã'] = phon_prefix + 'sase…æ…Ø'
        conjugations[text_prefix + '„Åó„Åü„Çâ'] = phon_prefix + '…ïita…æa'
    
    elif verb_type == 'kuru_compound':
        text_prefix = stems['text']['prefix']
        phon_prefix = stems['phoneme']['prefix']
        
        # Generate all Êù•„Çã forms with prefix
        conjugations[text_prefix + 'Êù•„Åü'] = phon_prefix + 'kita'
        conjugations[text_prefix + '„Åç„Åü'] = phon_prefix + 'kita'
        conjugations[text_prefix + 'Êù•„Å¶'] = phon_prefix + 'kite'
        conjugations[text_prefix + '„Åç„Å¶'] = phon_prefix + 'kite'
        conjugations[text_prefix + 'Êù•„Å™„ÅÑ'] = phon_prefix + 'konai'
        conjugations[text_prefix + '„Åì„Å™„ÅÑ'] = phon_prefix + 'konai'
        conjugations[text_prefix + 'Êù•„Å™„Åã„Å£„Åü'] = phon_prefix + 'konakatta'
        conjugations[text_prefix + '„Åì„Å™„Åã„Å£„Åü'] = phon_prefix + 'konakatta'
        conjugations[text_prefix + 'Êù•„Åæ„Åô'] = phon_prefix + 'kimas…Ø'
        conjugations[text_prefix + '„Åç„Åæ„Åô'] = phon_prefix + 'kimas…Ø'
        conjugations[text_prefix + 'Êù•„Åæ„Åó„Åü'] = phon_prefix + 'kima…ïita'
        conjugations[text_prefix + '„Åç„Åæ„Åó„Åü'] = phon_prefix + 'kima…ïita'
        conjugations[text_prefix + 'Êù•„Åæ„Åõ„Çì'] = phon_prefix + 'kimase…¥'
        conjugations[text_prefix + '„Åç„Åæ„Åõ„Çì'] = phon_prefix + 'kimase…¥'
        conjugations[text_prefix + 'Êù•„Åæ„Åõ„Çì„Åß„Åó„Åü'] = phon_prefix + 'kimase…¥de…ïita'
        conjugations[text_prefix + '„Åç„Åæ„Åõ„Çì„Åß„Åó„Åü'] = phon_prefix + 'kimase…¥de…ïita'
        conjugations[text_prefix + 'Êù•„Çå„Å∞'] = phon_prefix + 'k…Ø…æeba'
        conjugations[text_prefix + '„Åè„Çå„Å∞'] = phon_prefix + 'k…Ø…æeba'
        conjugations[text_prefix + 'Êù•„Çà„ÅÜ'] = phon_prefix + 'kojo…Ø'
        conjugations[text_prefix + '„Åì„Çà„ÅÜ'] = phon_prefix + 'kojo…Ø'
        conjugations[text_prefix + 'Êù•„ÅÑ'] = phon_prefix + 'koi'
        conjugations[text_prefix + '„Åì„ÅÑ'] = phon_prefix + 'koi'
        conjugations[text_prefix + 'Êù•„Çâ„Çå„Çã'] = phon_prefix + 'ko…æa…æe…æ…Ø'
        conjugations[text_prefix + '„Åì„Çâ„Çå„Çã'] = phon_prefix + 'ko…æa…æe…æ…Ø'
        conjugations[text_prefix + 'Êù•„Åï„Åõ„Çã'] = phon_prefix + 'kisase…æ…Ø'
        conjugations[text_prefix + '„Åì„Åï„Åõ„Çã'] = phon_prefix + 'kosase…æ…Ø'
        conjugations[text_prefix + 'Êù•„Åü„Çâ'] = phon_prefix + 'kita…æa'
        conjugations[text_prefix + '„Åç„Åü„Çâ'] = phon_prefix + 'kita…æa'
    
    return conjugations


def process_verb_batch(entries_batch):
    """
    Process a batch of dictionary entries to generate conjugations.
    Worker function for multiprocessing.
    
    Args:
        entries_batch: List of (word, phoneme) tuples
        
    Returns:
        tuple: (conjugations_dict, conjugated_words_set, verb_count)
    """
    batch_conjugations = {}
    batch_conjugated_words = set()
    batch_verb_count = 0
    
    for word, phoneme in entries_batch:
        verb_type = detect_verb_type(word, phoneme)
        
        if verb_type:
            batch_verb_count += 1
            
            try:
                conjugations = generate_conjugations(word, phoneme, verb_type)
                
                # Collect conjugations
                for conj_word, conj_phoneme in conjugations.items():
                    batch_conjugations[conj_word] = conj_phoneme
                    batch_conjugated_words.add(conj_word)
            
            except Exception as e:
                # Skip errors in worker
                continue
    
    return (batch_conjugations, batch_conjugated_words, batch_verb_count)


def convert_to_ligatures(phoneme_str):
    """Convert multi-char IPA sequences to single-char ligatures"""
    result = phoneme_str
    
    # Sort by length (longest first) to handle overlapping patterns
    for multi_char, ligature in sorted(LIGATURE_MAP.items(), key=lambda x: len(x[0]), reverse=True):
        result = result.replace(multi_char, ligature)
    
    return result


# ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
# BINARY TRIE BUILDER
# ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

class TrieNodeBuilder:
    """In-memory trie node for building binary trie"""
    def __init__(self):
        self.children = {}  # code_point -> TrieNodeBuilder
        self.value = None   # phoneme string or empty string for word markers
        self.offset = 0     # Will be set during serialization
        
    def insert(self, text, value=""):
        """Insert text with optional value (empty string for word markers)"""
        current = self
        
        # Convert text to Unicode code points
        # In Python 3, strings are already Unicode, so we just use ord() on each character
        code_points = [ord(c) for c in text]
        
        # Walk/create trie
        for cp in code_points:
            if cp not in current.children:
                current.children[cp] = TrieNodeBuilder()
            current = current.children[cp]
        
        current.value = value


def serialize_trie_node(node, output, offset_tracker):
    """
    Recursively serialize a trie node to binary format.
    Returns the offset where this node was written.
    """
    # Remember where we're writing this node
    node_offset = len(output)
    node.offset = node_offset
    
    # Flags byte
    has_value = 1 if node.value is not None else 0
    flags = has_value
    output.append(flags)
    
    # Value (if present)
    if node.value is not None:
        value_bytes = node.value.encode('utf-8')
        value_len = len(value_bytes)
        output.extend(struct.pack('<H', value_len))  # 2 bytes: value length
        output.extend(value_bytes)
    else:
        output.extend(struct.pack('<H', 0))  # No value
    
    # Children count
    num_children = len(node.children)
    output.extend(struct.pack('<I', num_children))  # 4 bytes: children count
    
    # Reserve space for children entries (we'll fill these in later)
    children_table_offset = len(output)
    # Each child entry: 4 bytes (code point) + 8 bytes (offset) = 12 bytes
    output.extend(b'\x00' * (num_children * 12))
    
    # Recursively serialize children and record their offsets
    child_offsets = {}
    for code_point, child_node in sorted(node.children.items()):
        child_offset = serialize_trie_node(child_node, output, offset_tracker)
        child_offsets[code_point] = child_offset
    
    # Now go back and fill in the children table
    table_pos = children_table_offset
    for code_point, child_offset in sorted(child_offsets.items()):
        # Write code point (4 bytes)
        struct.pack_into('<I', output, table_pos, code_point)
        table_pos += 4
        # Write offset (8 bytes)
        struct.pack_into('<Q', output, table_pos, child_offset)
        table_pos += 8
    
    return node_offset


def build_binary_trie(phoneme_dict, word_set, output_path):
    """
    Build a unified binary trie containing both phonemes and words.
    
    Args:
        phoneme_dict: Dictionary of {text: phoneme}
        word_set: Set of words (will be marked with empty string values)
        output_path: Path to write the .trie file
    """
    print(f"\nüî• Building unified binary trie...")
    
    # Create root node
    root = TrieNodeBuilder()
    
    # Insert phoneme entries
    print(f"   Inserting {len(phoneme_dict)} phoneme entries...")
    for idx, (text, phoneme) in enumerate(phoneme_dict.items()):
        root.insert(text, phoneme)
        if idx % 50000 == 0 and idx > 0:
            print(f"\r   Phonemes: {idx}/{len(phoneme_dict)}", end='', flush=True)
    print(f"\r   Phonemes: {len(phoneme_dict)}/{len(phoneme_dict)} ‚úì")
    
    # Insert word entries (with empty value as marker)
    # Only add words that aren't already phoneme entries to avoid duplicates
    word_only_count = 0
    print(f"   Inserting {len(word_set)} word entries...")
    for idx, word in enumerate(word_set):
        if word not in phoneme_dict:
            root.insert(word, "")  # Empty string marks word existence
            word_only_count += 1
        if idx % 50000 == 0 and idx > 0:
            print(f"\r   Words: {idx}/{len(word_set)} ({word_only_count} unique)", end='', flush=True)
    print(f"\r   Words: {len(word_set)}/{len(word_set)} ({word_only_count} unique) ‚úì")
    
    # Serialize to binary
    print(f"   Serializing trie to binary format...")
    output = bytearray()
    
    # Write header
    # Magic number: "JPNT" (Japanese Phoneme/Name Trie)
    output.extend(b'JPNT')
    
    # Version: 1.0
    output.extend(struct.pack('<H', 1))  # Major version
    output.extend(struct.pack('<H', 0))  # Minor version
    
    # Entry counts
    output.extend(struct.pack('<I', len(phoneme_dict)))  # Phoneme entries
    output.extend(struct.pack('<I', len(word_set)))       # Word entries
    
    # Root node offset (will be right after header)
    root_offset = len(output)
    output.extend(struct.pack('<Q', root_offset))  # 8 bytes: root offset
    
    # Serialize the trie
    offset_tracker = {'next': root_offset}
    serialize_trie_node(root, output, offset_tracker)
    
    # Write to file
    print(f"   Writing to {output_path}...")
    with open(output_path, 'wb') as f:
        f.write(output)
    
    file_size = len(output)
    file_size_mb = file_size / (1024 * 1024)
    
    print(f"   ‚úÖ Binary trie created!")
    print(f"   Size: {file_size:,} bytes ({file_size_mb:.2f} MB)")
    print(f"   Entries: {len(phoneme_dict)} phonemes + {word_only_count} words")
    
    return output_path

def main():
    # Use original_ja_phonemes.json as source
    source_file = 'original_ja_phonemes.json'
    if not os.path.exists(source_file):
        print(f"ERROR: {source_file} not found!")
        return
    
    print(f"Loading {source_file}...")
    with open(source_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    original_count = len(data)
    print(f"   Original entries: {original_count}")
    
    # Step 1: Add missing basic kana and common characters
    print("\nStep 1: Adding missing basic hiragana, katakana, and common characters...")
    added_count = 0
    
    for char, phoneme in {**BASIC_HIRAGANA, **BASIC_KATAKANA, **COMMON_KANJI}.items():
        if char not in data:
            data[char] = phoneme
            added_count += 1
    
    print(f"   Added {added_count} missing entries")
    
    # Step 1.5: Generate verb conjugations (PARALLELIZED)
    print("\nStep 1.5: Generating verb conjugations...")
    
    # Determine optimal number of worker processes
    num_workers = max(1, cpu_count() - 1)  # Leave one core free
    print(f"   Using {num_workers} worker processes for parallel processing")
    
    # Convert dictionary to list of tuples for processing
    all_entries = list(data.items())
    total_entries = len(all_entries)
    
    # Split entries into batches for workers
    batch_size = max(1000, total_entries // (num_workers * 4))  # Dynamic batch size
    batches = []
    for i in range(0, total_entries, batch_size):
        batches.append(all_entries[i:i + batch_size])
    
    print(f"   Processing {total_entries} entries in {len(batches)} batches...")
    print(f"   Batch size: {batch_size} entries per batch")
    
    # Track all conjugated words and stats
    conjugated_words = set()
    verb_count = 0
    conjugation_count = 0
    skipped_count = 0
    sample_verbs = []
    
    # Process batches in parallel with progress reporting
    print(f"\n   Progress: [", end='', flush=True)
    progress_bar_width = 40
    completed_batches = 0
    
    with Pool(processes=num_workers) as pool:
        # Process batches and show progress
        for batch_result in pool.imap_unordered(process_verb_batch, batches):
            batch_conjugations, batch_conjugated_words, batch_verb_count = batch_result
            
            # Merge results from this batch
            verb_count += batch_verb_count
            conjugated_words.update(batch_conjugated_words)
            
            # Add conjugations to dictionary (skip if already exists)
            for conj_word, conj_phoneme in batch_conjugations.items():
                if conj_word not in data:
                    data[conj_word] = conj_phoneme
                    conjugation_count += 1
                    
                    # Collect samples from first few conjugations
                    if len(sample_verbs) < 3 and len(batch_conjugations) > 0:
                        # Just note that we have samples (details come from first batches)
                        pass
                else:
                    skipped_count += 1
            
            # Update progress bar
            completed_batches += 1
            progress = completed_batches / len(batches)
            filled = int(progress_bar_width * progress)
            
            # Redraw progress bar
            print(f"\r   Progress: [{'=' * filled}{' ' * (progress_bar_width - filled)}] {progress * 100:.1f}% | Verbs: {verb_count} | Conjugations: {conjugation_count}", end='', flush=True)
    
    print()  # New line after progress bar
    print(f"\n   ‚úÖ Parallel processing complete!")
    print(f"   Found {verb_count} verbs")
    print(f"   Generated {conjugation_count} new conjugations")
    print(f"   Skipped {skipped_count} (already existed)")
    
    # Show sample conjugations
    if sample_verbs:
        print(f"\n   Sample verb conjugations:")
        for word, phoneme, vtype, count in sample_verbs:
            print(f"     ‚Ä¢ {word} ({phoneme}) [{vtype}] ‚Üí {count} forms")
    
    # Step 1.6: Update word list with conjugated forms
    print(f"\nStep 1.6: Updating word list...")
    
    # Check if original_ja_words.txt exists
    words_source = 'original_ja_words.txt'
    if os.path.exists(words_source):
        print(f"   Loading {words_source}...", end='', flush=True)
        
        # Use a set for fast duplicate detection
        word_set = set()
        
        # Load existing words with progress
        with open(words_source, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        for line in lines:
            word = line.strip()
            if word:
                word_set.add(word)
        
        original_word_count = len(word_set)
        print(f" Done! ({original_word_count} words)")
        
        # Add conjugated verb forms
        print(f"   Adding {len(conjugated_words)} conjugated forms...", end='', flush=True)
        words_added = 0
        for conj_word in conjugated_words:
            if conj_word not in word_set:
                word_set.add(conj_word)
                words_added += 1
        
        print(f" Done! (+{words_added} new)")
        print(f"   Total words: {len(word_set)}")
        
        # Save as ja_words.txt (sorted for consistency)
        print(f"   Sorting and saving ja_words.txt...", end='', flush=True)
        with open('ja_words.txt', 'w', encoding='utf-8') as f:
            for word in sorted(word_set):
                f.write(word + '\n')
        
        print(f" Done!")
        print(f"   ‚úÖ Word list saved to ja_words.txt")
    else:
        print(f"   ‚ö†Ô∏è  {words_source} not found, skipping word list update")
    
    # Step 2: Remove punctuation entries
    print("\nStep 2: Removing punctuation entries...")
    removed_punct = 0
    for punct in PUNCTUATION_TO_REMOVE:
        if punct in data:
            del data[punct]
            removed_punct += 1
    print(f"   Removed {removed_punct} punctuation entries")
    
    # Step 3: Convert to ligatures
    print("\nStep 3: Converting multi-char IPA to ligatures...")
    converted_count = 0
    total_keys = len(data)
    progress_interval = max(1, total_keys // 20)  # Update progress 20 times
    
    for idx, key in enumerate(data):
        original = data[key]
        converted = convert_to_ligatures(original)
        if original != converted:
            data[key] = converted
            converted_count += 1
        
        # Progress reporting
        if idx % progress_interval == 0:
            progress_pct = (idx / total_keys) * 100
            print(f"\r   Processing: {progress_pct:.1f}% ({idx}/{total_keys}) | Converted: {converted_count}", end='', flush=True)
    
    print(f"\r   ‚úÖ Converted {converted_count} entries" + " " * 40)  # Clear progress line
    
    # Show examples (skip console output due to encoding issues on Windows)
    print(f"\n   Ligature conversions completed")
    
    # Step 4: Validate against tokenizer vocab
    print(f"\nStep 4: Validating against tokenizer_vocab.json...")
    with open('tokenizer_vocab.json', 'r', encoding='utf-8') as f:
        tokenizer_vocab = json.load(f)
    
    valid_chars = set(tokenizer_vocab.keys())
    print(f"   Tokenizer has {len(valid_chars)} valid characters")
    
    # Check for invalid characters
    invalid_entries = []
    for key, phoneme in data.items():
        for char in phoneme:
            if char not in valid_chars:
                invalid_entries.append((key, phoneme, char))
                break
    
    if invalid_entries:
        print(f"   WARNING: Found {len(invalid_entries)} entries with invalid characters")
        print(f"   Writing details to invalid_phonemes.txt...")
        with open('invalid_phonemes.txt', 'w', encoding='utf-8') as f:
            for key, phoneme, invalid_char in invalid_entries:
                f.write(f"'{key}' -> '{phoneme}' (invalid: '{invalid_char}')\n")
    else:
        print(f"   [OK] All phonemes use valid tokenizer characters!")
    
    # Step 5: Save cleaned dictionary
    print(f"\nStep 5: Saving ja_phonemes.json...")
    print(f"   Final count: {len(data)} entries")
    with open('ja_phonemes.json', 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    # Step 6: Build unified binary trie
    print(f"\nStep 6: Building binary trie format...")
    if os.path.exists(words_source):
        build_binary_trie(data, word_set, 'japanese.trie')
    else:
        print(f"   ‚ö†Ô∏è  Skipping binary trie (word list not available)")
    
    print(f"\n[COMPLETE] Done!")
    print(f"\nSummary:")
    print(f"   - Original entries: {original_count}")
    print(f"   - Added missing kana/kanji: {added_count}")
    print(f"   - Verbs found: {verb_count}")
    print(f"   - Verb conjugations generated: {conjugation_count}")
    if os.path.exists(words_source):
        print(f"   - Word list: {original_word_count} ‚Üí {len(word_set)} (+{words_added})")
    print(f"   - Removed punctuation: {removed_punct}")
    print(f"   - Converted to ligatures: {converted_count}")
    print(f"   - Invalid phoneme entries: {len(invalid_entries)}")
    print(f"   - Final entries: {len(data)}")
    print(f"\nOutput files:")
    print(f"   - ja_phonemes.json (phoneme dictionary)")
    if os.path.exists(words_source):
        print(f"   - ja_words.txt (word segmentation dictionary)")
        print(f"   - japanese.trie (binary trie - FAST loading!)")
    print(f"\nNote: Punctuation in input text will pass through unchanged")
    print(f"Note: All verb conjugations (past, „Å¶-form, negative, etc.) are now in dictionary")
    print(f"Note: Use japanese.trie for 100x faster loading in C++!")

if __name__ == '__main__':
    main()

